<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>CTGAN 复述 - 博客 | C6H5-NO2</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="C6H5-NO2"><meta name=description content="本文对 CTGAN 的原理和实现进行简单复述。论文原文在 arXiv 上，参考的代码版本是 v0.2.2。
"><meta name=generator content="Hugo 0.82.0"><link rel=canonical href=https://www.c6h5no2.com/blog/archives/0003_ctgan_retold/><link rel=icon href=https://www.c6h5no2.com/blog/favicon.ico><link rel=stylesheet href=https://www.c6h5no2.com/blog/sass/jane.min.baad020e364947b4fe4fd989048956621a910c860671005660db5398659e66b5.css integrity="sha256-uq0CDjZJR7T+T9mJBIlWYhqRDIYGcQBWYNtTmGWeZrU=" media=screen crossorigin=anonymous><meta property="og:title" content="CTGAN 复述"><meta property="og:description" content="本文对 CTGAN 的原理和实现进行简单复述。论文原文在 arXiv 上，参考的代码版本是 v0.2.2。"><meta property="og:type" content="article"><meta property="og:url" content="https://www.c6h5no2.com/blog/archives/0003_ctgan_retold/"><meta property="article:section" content="post"><meta property="article:published_time" content="2020-12-21T00:00:00+00:00"><meta property="article:modified_time" content="2020-12-21T00:00:00+00:00"><meta itemprop=name content="CTGAN 复述"><meta itemprop=description content="本文对 CTGAN 的原理和实现进行简单复述。论文原文在 arXiv 上，参考的代码版本是 v0.2.2。"><meta itemprop=datePublished content="2020-12-21T00:00:00+00:00"><meta itemprop=dateModified content="2020-12-21T00:00:00+00:00"><meta itemprop=wordCount content="3090"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="CTGAN 复述"><meta name=twitter:description content="本文对 CTGAN 的原理和实现进行简单复述。论文原文在 arXiv 上，参考的代码版本是 v0.2.2。"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-105802886-1','auto'),ga('send','pageview'))</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=https://www.c6h5no2.com/blog/ class=logo>C6H5-NO2</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=https://www.c6h5no2.com/blog/archives/>归档</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://www.c6h5no2.com/friends/>人脉</a></li><li class=mobile-menu-item><a class=menu-item-link href=https://www.c6h5no2.com/about/>关于</a></li></ul></nav><header id=header class="header container"><div class=logo-wrapper><a href=https://www.c6h5no2.com/blog/ class=logo>C6H5-NO2</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=https://www.c6h5no2.com/blog/archives/>归档</a></li><li class=menu-item><a class=menu-item-link href=https://www.c6h5no2.com/friends/>人脉</a></li><li class=menu-item><a class=menu-item-link href=https://www.c6h5no2.com/about/>关于</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>CTGAN 复述</h1><div class=post-meta><time datetime=2020-12-21 class=post-time>2020-12-21</time></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#数据表示>数据表示</a><ul><li><a href=#实现细节>实现细节</a><ul><li><a href=#datatransformerfit>DataTransformer.fit()</a></li><li><a href=#datatransformertransform>DataTransformer.transform()</a></li></ul></li></ul></li><li><a href=#条件生成器>条件生成器</a><ul><li><a href=#实现细节-1>实现细节</a><ul><li><a href=#conditionalgenerator__init__>ConditionalGenerator.__init__()</a></li><li><a href=#conditionalgeneratorsample>ConditionalGenerator.sample()</a></li></ul></li></ul></li><li><a href=#采样器>采样器</a><ul><li><a href=#实现细节-2>实现细节</a></li></ul></li><li><a href=#训练过程>训练过程</a><ul><li><a href=#生成器>生成器</a></li><li><a href=#判别器>判别器</a></li><li><a href=#实现细节-3>实现细节</a></li></ul></li><li><a href=#采样合成>采样/合成</a></li></ul></nav></div></div><div class=post-content><p><span style=margin-left:2em>本文对 CTGAN 的原理和实现进行简单复述。论文原文在 <a href=https://arxiv.org/abs/1907.00503 target=_blank rel=noopener>arXiv</a> 上，参考的代码版本是 <a href=https://github.com/sdv-dev/CTGAN/tree/v0.2.2 target=_blank rel=noopener>v0.2.2</a>。</span></p><h1 id=数据表示>数据表示</h1><p>　　对连续列，采取模式特定的正则化（mode-specific normalization）。这种方法能够</p><blockquote><p>... convert continuous values of arbitrary range and distribution into a bounded vector representation suitable for neural networks. <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p></blockquote><p>这是因为</p><blockquote><p>Neural networks can effectively generate values with a distribution centered over (−1, 1) using tanh, as well as a low-cardinality multinomial distribution using softmax. <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p></blockquote><p>提出这个正则化的目标是把输入映射到 (-1, 1) 的区间上。</p><p>　　使用变分高斯混合模型（variational Gaussian mixture model），原始的分布可通过下述公式拟合<br><span style=margin-left:4em>$\displaystyle p(c)=\sum_{k=1}^{K}\pi_kN(c~\vert~\mu_k,\Sigma_k)$</span>
<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup><br>其中 $c$ 为某一连续列的某一值。注意到实际一列视为一个变量，协方差矩阵 $\Sigma$ 退化成方差 $\sigma^2$。</p><p>　　对拟合好的 VGM，可以计算任一值 $c$ 来自各个模式（mode，或作 state，component）的概率 $\rho_k$。对各个模式进行一次随机采样，各模式被采样的概率即是各自对应的 $\rho_k$。设模式 $k^*$ 被采样，则用该模式进行正则化。其中<br><span style=margin-left:4em>$\alpha=\cfrac{c-\mu_{k^*}}{4~\sigma_{k^*}}$<br></span><span style=margin-left:4em>$\beta$</span> 为表示 $k^*$ 被采样的 one-hot 向量<br>注意到极大多数情况下，$c$ 服从被选上的高斯分布，因而这个正则化是合理的。另外，在 TGAN 中，分母中的常数用的是 2。实际上，最后正则化的值会另被钳在 (-0.99, 0.99) 的区间中。</p><p>　　对离散列，则使用 one-hot 向量表示。</p><p>　　最终一行数据可表为<br><span style=margin-left:4em>$r=\alpha_1\oplus\beta_1\oplus\cdots\oplus\alpha_{N_c}\oplus\beta_{N_c}\oplus d_1\oplus\cdots\oplus d_{N_d}$</span></p><h2 id=实现细节>实现细节</h2><p>　　先把 <code>train_data</code> 过一遍 <code>DataTransformer</code>，分 <code>fit</code> 和 <code>transform</code> 两步。</p><h3 id=datatransformerfit>DataTransformer.fit()</h3><p>　　<code>fit()</code> 中对数据产生了如下 meta 信息。</p><ul><li>连续列</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-py data-lang=py><span class=c1># transformer.py Line 30</span>
<span class=p>{</span>
    <span class=s1>&#39;name&#39;</span><span class=p>:</span> <span class=n>column</span><span class=p>,</span>
    <span class=s1>&#39;model&#39;</span><span class=p>:</span> <span class=n>gm</span><span class=p>,</span>
    <span class=s1>&#39;components&#39;</span><span class=p>:</span> <span class=n>components</span><span class=p>,</span>
    <span class=s1>&#39;output_info&#39;</span><span class=p>:</span> <span class=p>[(</span><span class=mi>1</span><span class=p>,</span> <span class=s1>&#39;tanh&#39;</span><span class=p>),</span> <span class=p>(</span><span class=n>num_components</span><span class=p>,</span> <span class=s1>&#39;softmax&#39;</span><span class=p>)],</span>
    <span class=s1>&#39;output_dimensions&#39;</span><span class=p>:</span> <span class=mi>1</span> <span class=o>+</span> <span class=n>num_components</span><span class=p>,</span>
<span class=p>}</span>
</code></pre></td></tr></table></div></div><p><sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> 其中 <code>components</code> 为数组表示 VGM 中假设的各个分量是否激活，<code>num_components</code> 表示激活的分量总数。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-py data-lang=py><span class=n>gm</span> <span class=o>=</span> <span class=n>BayesianGaussianMixture</span><span class=p>(</span>
    <span class=n>n_components</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>n_clusters</span><span class=p>,</span>  <span class=c1># = 10</span>
    <span class=n>weight_concentration_prior_type</span><span class=o>=</span><span class=s1>&#39;dirichlet_process&#39;</span><span class=p>,</span>
    <span class=n>weight_concentration_prior</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span>
    <span class=n>n_init</span><span class=o>=</span><span class=mi>1</span>
<span class=p>)</span>
</code></pre></td></tr></table></div></div><p><sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> 重要的是 <code>weight_concentration_prior</code> 参数。通过指定一个低浓度先验，将会使模型将大部分的权重分配到少数分量上，而其余分量的权重则趋近 0 <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>。之后调用 <code>BayesianGaussianMixture.fit()</code> 使用 EM 算法估计模型的参数，上述 <code>weight_concentration_prior</code> 即对应 Bishop 书中的 $\gamma$。</p><ul><li>离散列</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-py data-lang=py><span class=p>{</span>
    <span class=s1>&#39;name&#39;</span><span class=p>:</span> <span class=n>column</span><span class=p>,</span>
    <span class=s1>&#39;encoder&#39;</span><span class=p>:</span> <span class=n>ohe</span><span class=p>,</span>  <span class=c1># ohe = OneHotEncodingTransformer()</span>
    <span class=s1>&#39;output_info&#39;</span><span class=p>:</span> <span class=p>[(</span><span class=n>categories</span><span class=p>,</span> <span class=s1>&#39;softmax&#39;</span><span class=p>)],</span>
    <span class=s1>&#39;output_dimensions&#39;</span><span class=p>:</span> <span class=n>categories</span>
<span class=p>}</span>
</code></pre></td></tr></table></div></div><p><sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> 即，使用 one-hot 向量表示，<code>categories</code> 为该列的类别（category）数。</p><p>　　上述两种结构都推入 <code>meta</code>。</p><h3 id=datatransformertransform>DataTransformer.transform()</h3><p>　　包含 <code>_transform_continuous()</code> 和 <code>_transform_discrete()</code>。后者是简单的变为 one-hot 向量，前者实现上有些操作。</p><p>　　<code>data</code> 的形状为 (len, 1)，其中 len 为数据集的行数。<code>features</code> 的形状为 (len, n_clusters)。随机选取的时候，随机选取的概率为 Bayesian Gaussian Mixture 预测的概率加上 1e-6 再取概率。最后输出 <code>features</code>（即 $\alpha$）时，会 <code>np.clip()</code> 到 (-.99, .99) 的区间中。</p><p>　　用正态分布构造示例数据，经 transfrom 得样例输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-py data-lang=py><span class=p>[[</span><span class=o>-</span><span class=mf>0.15475138</span>  <span class=mf>1.</span>  <span class=mf>0.</span>  <span class=mf>0.</span>     <span class=mf>0.0182173</span>   <span class=mf>1.</span>    <span class=mf>1.</span>  <span class=mf>0.</span>  <span class=mf>0.</span>    <span class=mf>1.</span>  <span class=mf>0.</span><span class=p>]</span>
 <span class=p>[</span><span class=o>-</span><span class=mf>0.1148458</span>   <span class=mf>0.</span>  <span class=mf>0.</span>  <span class=mf>1.</span>    <span class=o>-</span><span class=mf>0.35653856</span>  <span class=mf>1.</span>    <span class=mf>1.</span>  <span class=mf>0.</span>  <span class=mf>0.</span>    <span class=mf>1.</span>  <span class=mf>0.</span><span class=p>]</span>
 <span class=p>[</span><span class=o>-</span><span class=mf>0.1148458</span>   <span class=mf>0.</span>  <span class=mf>0.</span>  <span class=mf>1.</span>    <span class=o>-</span><span class=mf>0.08588155</span>  <span class=mf>1.</span>    <span class=mf>1.</span>  <span class=mf>0.</span>  <span class=mf>0.</span>    <span class=mf>1.</span>  <span class=mf>0.</span><span class=p>]</span>
 <span class=p>[</span> <span class=mf>0.15728488</span>  <span class=mf>0.</span>  <span class=mf>0.</span>  <span class=mf>1.</span>     <span class=mf>0.20559523</span>  <span class=mf>1.</span>    <span class=mf>0.</span>  <span class=mf>1.</span>  <span class=mf>0.</span>    <span class=mf>1.</span>  <span class=mf>0.</span><span class=p>]</span>
 <span class=p>[</span> <span class=mf>0.02121954</span>  <span class=mf>0.</span>  <span class=mf>0.</span>  <span class=mf>1.</span>     <span class=mf>0.37215339</span>  <span class=mf>1.</span>    <span class=mf>0.</span>  <span class=mf>1.</span>  <span class=mf>0.</span>    <span class=mf>1.</span>  <span class=mf>0.</span><span class=p>]</span>
 <span class=p>[</span><span class=o>-</span><span class=mf>0.17555746</span>  <span class=mf>0.</span>  <span class=mf>1.</span>  <span class=mf>0.</span>    <span class=o>-</span><span class=mf>0.08588155</span>  <span class=mf>1.</span>    <span class=mf>1.</span>  <span class=mf>0.</span>  <span class=mf>0.</span>    <span class=mf>0.</span>  <span class=mf>1.</span><span class=p>]</span>
 <span class=p>[</span> <span class=mf>0.5654809</span>   <span class=mf>0.</span>  <span class=mf>0.</span>  <span class=mf>1.</span>     <span class=mf>0.12231615</span>  <span class=mf>1.</span>    <span class=mf>1.</span>  <span class=mf>0.</span>  <span class=mf>0.</span>    <span class=mf>0.</span>  <span class=mf>1.</span><span class=p>]</span>
 <span class=p>[</span> <span class=mf>0.24642384</span>  <span class=mf>0.</span>  <span class=mf>1.</span>  <span class=mf>0.</span>     <span class=mf>0.47625224</span>  <span class=mf>1.</span>    <span class=mf>0.</span>  <span class=mf>1.</span>  <span class=mf>0.</span>    <span class=mf>0.</span>  <span class=mf>1.</span><span class=p>]</span>
 <span class=p>[</span><span class=o>-</span><span class=mf>0.00676494</span>  <span class=mf>0.</span>  <span class=mf>1.</span>  <span class=mf>0.</span>    <span class=o>-</span><span class=mf>0.16916063</span>  <span class=mf>1.</span>    <span class=mf>0.</span>  <span class=mf>0.</span>  <span class=mf>1.</span>    <span class=mf>0.</span>  <span class=mf>1.</span><span class=p>]</span>
 <span class=p>[</span><span class=o>-</span><span class=mf>0.04896307</span>  <span class=mf>0.</span>  <span class=mf>1.</span>  <span class=mf>0.</span>    <span class=o>-</span><span class=mf>0.06506178</span>  <span class=mf>1.</span>    <span class=mf>1.</span>  <span class=mf>0.</span>  <span class=mf>0.</span>    <span class=mf>0.</span>  <span class=mf>1.</span><span class=p>]]</span>
</code></pre></td></tr></table></div></div><p>依次为连续列 $C_1$、连续列 $C_2$、离散列 $D_1$、离散列 $D_2$。$C_1$ 中采样到 3 个模式；$C_2$ 中 1 个；$D_1$ 有 3 个类别；$D_2$ 有 2 个。</p><h1 id=条件生成器>条件生成器</h1><p>　　引入条件生成器（conditional generator）以期望解决离散列的类别不平衡（class imbalance）问题。该生成器的输出是一个 one-hot 形式的条件向量（conditional vector），其长度为各离散列类别数之和，即 $\sum_{i=1}^{N_d}|D_i|$。这个条件向量是随机的，其各个离散列的可能性均等，但列内各类别的可能性与数据集的相同。</p><p>　　条件生成器使得生成器原本要拟合的左端项<br><span style=margin-left:4em>$\displaystyle P(\text{row})=\sum_{k\in D_i} P(\text{row}|D_i=k)P(D_i=k)$</span> <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup><br>变为在给定 $D_i=k$ 的条件下拟合 $P(\text{row}|D_i=k)$，以期小模式也被充分采样。</p><p>　　注意这并非 GAN 意义下的生成器，而更类似于采样器。v0.3.0 版本中与采样器合并。</p><h2 id=实现细节-1>实现细节</h2><p>　　本小节中设数组下标从 1 开始。</p><p>　　记离散列数为 $N_d$，第 $i$ 个离散列 $D_i$ 的类别（category）数为 $|D_i|$。记数据集行数为 <code>len</code>，批大小为 <code>batch</code>。</p><h3 id=conditionalgenerator__init__>ConditionalGenerator.__init__()</h3><p>　　构造函数中，对数据集进行处理。<code>n_col</code> 记录离散列数 $N_d$。<code>n_opt</code> 记录各离散列类别数之和，即 $\sum_{i=1}^{N_d}|D_i|$。<code>model</code> 形状为 <code>(n_col, len)</code>，其每一列表示数据集对应行对应的离散类别序号；这个变量只在整个模型训练完毕后，生成数据时在不传入条件向量的情况下使用。<code>interval</code> 形状为 <code>(n_col, 2)</code>，意义为<br><span style=margin-left:4em>$\displaystyle \text{interval}[j] = \left( \sum_{k=0}^{j-1} |D_k| , \; |D_{j}| \right) \quad (j=1,2,\cdots,N_d, \; |D_0|=0)$</span></p><p>　　重要的是计算出 <code>p</code>，它表征了离散列在数据集中的先验。其形状为 <code>(n_col, max_interval)</code>，其中 <code>max_interval</code> 表示 $\max|D_i|$。对第 $i$ 行，其第 $1\le j\le|D_i|$ 列表示离散列 $D_i$ 取其第 $j$ 个类别的对数频率（log frequency），余下的列均为 0。</p><h3 id=conditionalgeneratorsample>ConditionalGenerator.sample()</h3><p>　　通过 <code>sample()</code> 采样。先在 $[1,N_d]$ 的整数中等可能地随机抽取，生成长度为 <code>batch</code> 的序列 <code>idx</code>，表示 <code>batch</code> 次随机到的离散列的序号。<code>mask1</code> 的形状为 <code>(batch, n_col)</code>，每行为 one-hot 向量，表示该次随机到的离散列。</p><p>　　另有序列 <code>opt1prime</code> 长度为 <code>batch</code>，表示从 <code>idx</code> 对应离散列中随机到的类别的（列内的）序号。具体方法是从 $U(0,1)$ 中采样 <code>r</code>，根据 <code>p</code> 的累积分布函数选择 $\underset{j}{\argmin}\operatorname{CDF}_p(j)>r$ 的类别。</p><p>　　最后 <code>vec1</code> 形状为 <code>(batch, n_opt)</code>，各行是对应的条件向量 <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>。</p><p>　　与训练中变量的对应关系：<br>　　　　<code>c1, m1, col, opt = vec1, mask1, idx, opt1prime</code> <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></p><h1 id=采样器>采样器</h1><p>　　采样器用于从数据集中采样。v0.3.0 版本中与条件生成器合并。</p><h2 id=实现细节-2>实现细节</h2><p>　　<code>Sampler.model</code> 为三维列表，第一维长度是离散列数 $N_d$，第二维长度是对应列的类别数 $|D_i|$，第三维存储离散列 $D_i$ 取类别 $j$ 的行的序号。</p><p>　　采样时，<code>Sampler.sample()</code> 根据给定的 <code>col</code> 和 <code>opt</code>，从满足该条件的行中随机等可能选取一行。</p><h1 id=训练过程>训练过程</h1><h2 id=生成器>生成器</h2><p>　　随机产生某定长向量 $z$，使各分量独立地采样自 $N(0,1)$。通过条件生成器采样向量 $cond$，其长度为各离散列的类别数之和。输入网络<br><span style=margin-left:4em>$h_0=z\oplus cond$</span>
<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup><br>定义<br><span style=margin-left:4em>$\operatorname{Residual}_{i\rarr o}(h)=[\operatorname{ReLU}\circ\operatorname{BatchNorm1d}\circ\operatorname{Linear}_{i\rarr o}(h)]\oplus h$<br></span>则<br><span style=margin-left:4em>$h_1=\operatorname{Residual}_{\lVert h_0\rVert\rarr 256}(h_0)$<br></span><span style=margin-left:4em>$h_2=\operatorname{Residual}_{\lVert h_1\rVert\rarr 256}(h_1)$<br></span><span style=margin-left:4em>$\operatorname{G}(\cdot)=\operatorname{Linear}_{\lVert h_2\rVert\rarr\lVert r\rVert}(h_2)$<br></span>生成器输出<br><span style=margin-left:4em>$r=\alpha_1\oplus\beta_1\oplus\cdots\oplus\alpha_{N_c}\oplus\beta_{N_c}\oplus d_1\oplus\cdots\oplus d_{N_d}$<br></span>再做<br><span style=margin-left:4em>$\hat \alpha_i=\tanh(\alpha_i)$<br></span><span style=margin-left:4em>$\hat \beta_i=\operatorname{gumbel}_{0.2}(\beta_i)$<br></span><span style=margin-left:4em>$\hat d_i=\operatorname{gumbel}_{0.2}(d_i)$<br></span>得到最终 $\hat r$。</p><p>　　损失函数的定义基于 WGAN<br><span style=margin-left:4em>$-E[D(\hat x)]$</span>
<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup><br>再加上 $\hat r$ 中的 $\hat d$ 中被 $mask$ 部分的交叉熵<br><span style=margin-left:4em>$loss=-E[D(\hat x)]+E[\operatorname{CrossEntropy}_{mask}(\hat d,cond)]$</span>
<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup><br>注意实际输入的 $x$ 包括行向量 $r$ 和条件向量 $cond$。</p><h2 id=判别器>判别器</h2><p>　　参考 PacGAN，将 pac 个样本作为一个包（packet），以期防止模式坍缩（mode collapse）。<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup><br><span style=margin-left:4em>$h_0=r_1\oplus cond_1\oplus\cdots\oplus r_{pac}\oplus cond_{pac}$<br></span><span style=margin-left:4em>$h_1=\operatorname{Dropout}_{0.5}\circ\operatorname{LeakyReLU}_{0.2}\circ\operatorname{Linear}_{\lVert h_0\rVert\rarr 256}(h_0)$<br></span><span style=margin-left:4em>$h_2=\operatorname{Dropout}_{0.5}\circ\operatorname{LeakyReLU}_{0.2}\circ\operatorname{Linear}_{\lVert h_1\rVert\rarr 256}(h_1)$<br></span><span style=margin-left:4em>$\operatorname{D}(\cdot)=\operatorname{Linear}_{\lVert h_2\rVert\rarr1}(h_2)$</span></p><p>　　损失函数的定义参考 WGAN-GP，在 WGAN 原本的损失函数的基础上加入梯度惩罚（gradient penalty）以使训练收敛<br><span style=margin-left:4em>$loss=E_{\hat x\sim P_{gen}}[D(\hat x)]-E_{x\sim P_{real}}[D(x)]+\lambda E_{\tilde x\sim P_{pen}}[\lVert\nabla_{\tilde x}D(\tilde x)\rVert_2-1]^2$</span>
<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup><br>其中 $\lambda=10$，$P_{pen}$ 采样自 $P_{gen}$ 到 $P_{real}$ 的随机线性内插值。注意实际输入的 $x$ 包括行向量 $r$ 和条件向量 $cond$。</p><h2 id=实现细节-3>实现细节</h2><p>　　<code>fakez</code> 的形状为 <code>(batch_size, embedding_dim)</code> 即 (500, 128)，各元素独立地采样自 $N(0,1)$。<code>condvec</code> 采样自条件生成器，四个项分别作 <code>c1, m1, col, opt</code>。将 <code>c1</code> 接到 <code>fakez</code> 之后。</p><p>　　生成随机置换 <code>perm</code>，将其作用于 <code>col, opt, c1</code>。从数据集中采样 <code>batch_size</code> 个符合置换后的 <code>col, opt</code> 的行 <code>real</code>；将置换后的 <code>c1</code>（记为 <code>c2</code>）接在其后，得 <code>real_cat</code>。</p><p>　　生成器输入长度为 <code>embedding_dim + n_opt</code>。<code>fake = generator(fakez)</code>。对 <code>fake</code> 中的对应分量施加函数 <code>_apply_activate()</code>，再接上 <code>c1</code> 得 <code>fake_cat</code>。</p><p>　　计算 <code>y_fake = discriminator(fake_cat)</code> 和 <code>y_real = discriminator(real_cat)</code>。</p><p>　　梯度惩罚的计算中，注意来自同一个包的样本插值量相同。</p><p>　　优化器为 Adam，学习率为 2e-4。</p><h1 id=采样合成>采样/合成</h1><p>　　若给出 <code>cond</code> 则将 <code>cond</code> 作为条件输入生成器；若不给定 <code>cond</code>，则按「各个离散列的可能性均等，但列内各类别的可能性与数据集的相同」的方式产生 <code>cond</code>。</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Lei Xu <em>et al</em>. Modeling tabular data using conditional gan. In <em>Advances in Neural Information Processing Systems</em>, 2019. <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>Lei Xu and Kalyan Veeramachaneni. Synthesizing tabular data using generative adversarial networks. <em>arXiv preprint arXiv:1811.11264</em>, 2018. <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>Christopher M Bishop. Pattern recognition and machine learning. <em>springer</em>, 2006. <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>https://github.com/sdv-dev/CTGAN/tree/v0.2.2 <a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p>https://scikit-learn.org/stable/modules/mixture.html#bgmm <a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6 role=doc-endnote><p>Martin Arjovsky <em>et al</em>. Wasserstein GAN. In <em>International Conference on Machine Learning</em>, 2017. <a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7 role=doc-endnote><p>Zinan Lin <em>et al</em>. PacGAN: The power of two samples in generative adversarial networks. In <em>Advances in Neural Information Processing Systems</em>, 2018. <a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8 role=doc-endnote><p>Ishaan Gulrajani <em>et al</em>. Improved Training of Wasserstein GANs. In <em>Advances in Neural Information Processing Systems</em>, 2017. <a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><footer class=post-footer style=margin-top:70px><nav class=post-nav><a class=next href=https://www.c6h5no2.com/blog/archives/0002_rebirth_2020/><span class="next-text nav-default">复更杂记</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697c-11.777231-11.500939-30.216186-10.304694-41.178865 2.712784z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io target=_blank>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme based on <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane target=_blank>Jane</a></span>
<span class=division>|</span>
<span class=theme-info>Licensed under <a class=theme-link href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=https://www.c6h5no2.com/blog/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=https://www.c6h5no2.com/blog/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=https://www.c6h5no2.com/blog/js/main.dee43230127a73d039a734510fa896c89c3c7ce0cf0be0c7a7433f8fd69b76dc.js integrity="sha256-3uQyMBJ6c9A5pzRRD6iWyJw8fODPC+DHp0M/j9abdtw=" crossorigin=anonymous></script><script type=text/javascript>window.MathJax={showProcessingMessages:!0}</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css integrity=sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js integrity=sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></body></html>